{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utility05_Bayesian import *\n",
    "# from bayes_opt import BayesianOptimization\n",
    "from tiramisu_net import *\n",
    "import scipy.io\n",
    "import cv2\n",
    "import multiprocessing\n",
    "from OsUtils import *\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import cv2\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_MoteCarlo_Samples=20\n",
    "def Monty_Model_softmax_X(X,model, N_MCS):\n",
    "    SoftMaxBin_list=[]\n",
    "    for i in range(N_MCS):\n",
    "        SoftMaxBin_list.append(model.predict(X))\n",
    "    SoftMaxBin=np.array(SoftMaxBin_list)\n",
    "    \n",
    "    SoftmaxMean=np.mean(SoftMaxBin,axis=0)\n",
    "    SoftmaxStd=np.std(SoftMaxBin,axis=0)\n",
    "\n",
    "    return SoftmaxMean,SoftmaxStd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meta_info import *\n",
    "N_MCS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_unique:\n",
      "[0. 1.]\n",
      "pixel_count:\n",
      "[19097025   659775]\n",
      "class_weights_freq:\n",
      "[1. 1.]\n"
     ]
    }
   ],
   "source": [
    "MFW_data=np.load(cd_str+'ZL_dataset/'+str(new_H)+'x'+str(new_W)+'_'+data_type+'_MFW_data.npz')\n",
    "\n",
    "pixel_unique=MFW_data['pixel_unique']\n",
    "pixel_count=MFW_data['pixel_count']\n",
    "class_weights_freq=MFW_data['class_weights_freq']\n",
    "\n",
    "print('pixel_unique:')\n",
    "print(pixel_unique)\n",
    "\n",
    "print('pixel_count:')\n",
    "pixel_count[1]=sum(pixel_count[1:])\n",
    "pixel_count=pixel_count[0:5]\n",
    "print(pixel_count)\n",
    "pixel_med=np.median(pixel_count)\n",
    "class_weights_freq=pixel_med/pixel_count\n",
    "\n",
    "class_weights_freq=class_weights_freq*0+1\n",
    "print('class_weights_freq:')\n",
    "print(class_weights_freq)\n",
    "\n",
    "Big_data_dictionary=load_pickle(cd_str+'ZL_dataset/Big_data_dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class only helps visualize the data and is not a generator\n",
    "class ZL_data_split:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 data_split_dict,split_type,split_id,train_sp,\n",
    "                 new_H=new_H,new_W=new_W):\n",
    "        \n",
    "        self.data_split_dict=data_split_dict\n",
    "        self.split_id=split_id      # 'train', 'val', 'test'\n",
    "        self.split_type=split_type  # 'tool', 'agent', 'setup', 'random'\n",
    "        self.new_H = new_H\n",
    "        self.new_W = new_W        \n",
    "        \n",
    "        self.split_key=self.split_id+'_indx_bin'\n",
    "        self.vid_frm_id_bin=self.data_split_dict[self.split_type][self.split_key]\n",
    "        \n",
    "        self.n_obs = len(self.vid_frm_id_bin)\n",
    "        self.n_first=0\n",
    "        self.n_last=self.n_obs\n",
    "        \n",
    "        if self.split_id=='train':\n",
    "            if train_sp=='sp1':\n",
    "                self.n_first=0\n",
    "                self.n_last=int(len(self.vid_frm_id_bin)/2)\n",
    "                self.n_obs=len(self.vid_frm_id_bin[self.n_first:self.n_last])                \n",
    "            elif train_sp=='sp2':\n",
    "                self.n_first=int(len(self.vid_frm_id_bin)/2)\n",
    "                self.n_last=len(self.vid_frm_id_bin)\n",
    "                self.n_obs=len(self.vid_frm_id_bin[self.n_first:self.n_last]) \n",
    "            elif train_sp=='all':\n",
    "                pass\n",
    "            else:\n",
    "                print('invalid sp')\n",
    "    \n",
    "    def load_data(self,image_id, resize=True,verbose=0,\n",
    "                  norm_RGB=255):\n",
    "        \n",
    "        split_key=self.split_id+'_indx_bin'\n",
    "        vid_frm_id_bin=self.data_split_dict[self.split_type][split_key][image_id]\n",
    "        \n",
    "        data_tmp=np.load(cd_str+'ZL_dataset/proc_'+str(self.new_H)+'x'+str(self.new_W)\\\n",
    "                                                    +'/'+vid_frm_id_bin+'.npz')\n",
    "        img_i=data_tmp['X']/norm_RGB\n",
    "        mask_i=data_tmp['Y']\n",
    "\n",
    "#         if resize:\n",
    "#             mask_i = cv2.resize(np.array(mask_i), (self.new_W,self.new_H), interpolation =cv2.INTER_NEAREST)\n",
    "#             img_i = cv2.resize(np.array(img_i), (self.new_W,self.new_H), interpolation =cv2.INTER_NEAREST)\n",
    "            \n",
    "        if verbose==1:\n",
    "            print('img_i.shape:  '+str(img_i.shape))\n",
    "            print('mask_i.shape: '+str(mask_i.shape))\n",
    "            \n",
    "        return mask_i,img_i    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set=ZL_data_split(Big_data_dictionary,data_type,'train','all',new_W=new_W,new_H=new_H)\n",
    "val_set=ZL_data_split(Big_data_dictionary,data_type,'val','none',new_W=new_W,new_H=new_H)\n",
    "test_set=ZL_data_split(Big_data_dictionary,data_type,'test','none',new_W=new_W,new_H=new_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator_B(tf.keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, class_weights, batch_size,\n",
    "                 data_split_dict,split_type,split_id,train_sp,\n",
    "                 n_classes=2, shuffle=True, new_H=new_H, new_W=new_W):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.class_weights = class_weights\n",
    "        self.n_classes = n_classes\n",
    "        self.data_split_dict=data_split_dict\n",
    "        self.split_type=split_type\n",
    "        self.split_id=split_id\n",
    "        self.train_sp=train_sp\n",
    "        self.shuffle = shuffle\n",
    "        self.new_H = new_H\n",
    "        self.new_W = new_W      \n",
    "        self.set_dir='SRG_all_'+self.split_id+'/'\n",
    "\n",
    "        self.split_key=self.split_id+'_indx_bin'\n",
    "        self.vid_frm_id_bin=self.data_split_dict[self.split_type][self.split_key]\n",
    "        self.n_obs = len(self.vid_frm_id_bin)\n",
    "        self.n_first=0\n",
    "        self.n_last=self.n_obs\n",
    "        \n",
    "        if self.split_id=='train':\n",
    "            if train_sp=='sp1':\n",
    "                self.n_first=0\n",
    "                self.n_last=int(len(self.vid_frm_id_bin)/2)\n",
    "                self.n_obs=len(self.vid_frm_id_bin[self.n_first:self.n_last])                \n",
    "            elif train_sp=='sp2':\n",
    "                self.n_first=int(len(self.vid_frm_id_bin)/2)\n",
    "                self.n_last=len(self.vid_frm_id_bin)\n",
    "                self.n_obs=len(self.vid_frm_id_bin[self.n_first:self.n_last]) \n",
    "            elif train_sp=='all':\n",
    "                pass\n",
    "            else:\n",
    "                print('invalid sp')       \n",
    "        \n",
    "        self.on_epoch_end()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(self.n_obs / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "        data_tuple = self.__data_generation(indexes)\n",
    "\n",
    "        return data_tuple\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.n_first,self.n_last,1) # only train on the first half of training set\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        batch_size_auto=indexes.shape[0]\n",
    "        X = np.empty((batch_size_auto, self.new_H,self.new_W, 8))\n",
    "        y = np.empty((batch_size_auto, self.new_H,self.new_W), dtype=int)\n",
    "        batch_weights = np.empty((batch_size_auto, self.new_H*self.new_W))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(indexes):\n",
    "            # Store sample\n",
    "            vid_frm_id=self.vid_frm_id_bin[ID]\n",
    "            X[i,] = np.load(self.set_dir+'X_SRG_'+vid_frm_id+'.npz')['X_SRG'] # already normalized\n",
    "            # Store class\n",
    "            data_tmp=np.load(cd_str+'ZL_dataset/proc_'+str(self.new_H)+'x'+str(self.new_W)\\\n",
    "                                                        +'/'+vid_frm_id+'.npz')\n",
    "            y[i]=data_tmp['Y']     \n",
    "            \n",
    "            avg_std=np.mean(X[i,:,:,5:7],axis=-1) # avg of softmax stds\n",
    "            RLmat=avg_std+X[i,:,:,7]        # add entropy\n",
    "            RLC=200\n",
    "                                 \n",
    "            batch_weights[i,]=np.reshape(RLmat,(1,new_H*self.new_W))*RLC\n",
    "             \n",
    "        #y[y>1]=1\n",
    "        y_resh=y.reshape((-1,self.new_H*self.new_W))\n",
    "#         print(np.amax(y_resh))\n",
    "#         print(y_resh.shape)\n",
    "        y_cat=tf.keras.utils.to_categorical(y_resh, num_classes=self.n_classes)\n",
    "#         batch_wieghts = gen_weight_mat(y,self.class_weights)\n",
    "#         print(batch_wieghts.shape)\n",
    "        return (X, y_cat,batch_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SRG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename_save = 'SRG_MW.h5'\n",
    "batch_size_eval=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MFW_data=np.load(cd_str+'ZL_dataset/'+str(new_H)+'x'+str(new_W)+'_'+data_type+'_MFW_data.npz')\n",
    "\n",
    "# pixel_unique=MFW_data['pixel_unique']\n",
    "# pixel_count=MFW_data['pixel_count']\n",
    "# class_weights_freq=MFW_data['class_weights_freq']\n",
    "\n",
    "# print('pixel_unique:')\n",
    "# print(pixel_unique)\n",
    "\n",
    "# print('pixel_count:')\n",
    "# pixel_count[1]=sum(pixel_count[1:])\n",
    "# pixel_count=pixel_count[0:5]\n",
    "# print(pixel_count)\n",
    "# pixel_med=np.median(pixel_count)\n",
    "# class_weights_freq=pixel_med/pixel_count\n",
    "\n",
    "# class_weights_freq=class_weights_freq*0+1\n",
    "# print('class_weights_freq:')\n",
    "# print(class_weights_freq)\n",
    "\n",
    "# Big_data_dictionary=load_pickle(cd_str+'ZL_dataset/Big_data_dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_generator_sp_all = DataGenerator_B(\n",
    "#                                     class_weights_freq,batch_size_train,                 \n",
    "#                                     Big_data_dictionary,data_type,'train','all',\n",
    "#                                     new_W=new_W,new_H=new_H)\n",
    "# val_generator = DataGenerator_B(\n",
    "#                               class_weights_freq,batch_size_eval,\n",
    "#                               Big_data_dictionary,data_type,'val','none',\n",
    "#                               new_W=new_W,new_H=new_H,\n",
    "#                               shuffle=False)\n",
    "test_generator = DataGenerator_B(\n",
    "                               class_weights_freq,batch_size_eval,\n",
    "                               Big_data_dictionary,data_type,'test','none',\n",
    "                               new_W=new_W,new_H=new_H,\n",
    "                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model=load_model(model_filename_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Monty_Model_softmax(data_generator, model, N_MCS=N_MCS,steps=100):\n",
    "    SoftMaxBin_list=[]\n",
    "    for i in range(N_MCS):\n",
    "        print(i)\n",
    "        SoftMaxBin_list.append(model.predict(data_generator))\n",
    "    SoftMaxBin=np.array(SoftMaxBin_list)\n",
    "    SoftmaxMean=np.mean(SoftMaxBin,axis=0)\n",
    "    SoftmaxStd=np.std(SoftMaxBin,axis=0)\n",
    "    return SoftmaxMean,SoftmaxStd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "SoftmaxMean_test,SoftmaxStd_test = Monty_Model_softmax(test_generator,best_model, N_MCS=N_MCS,steps=test_generator.n_obs/batch_size_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Y(set_object):\n",
    "\n",
    "    n_obs=set_object.n_obs\n",
    "    y=np.empty((set_object.n_obs, new_H,new_W), dtype=int)\n",
    "    for i_obs in range(set_object.n_obs):\n",
    "        mask_i,_ = set_object.load_data(i_obs,verbose=0,resize=True)\n",
    "        y[i_obs] = mask_i\n",
    "\n",
    "#    y[y>1]=1\n",
    "#     y_resh=y.reshape((-1,new_H*new_W))\n",
    "    print(y.shape)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gt_test=get_Y(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors\n",
    "\n",
    "for i_obs in np.arange(0,test_generator.n_obs,10):\n",
    "    obs_id_eval=i_obs\n",
    "\n",
    "    mask_i,img_i = test_set.load_data(obs_id_eval,verbose=0,resize=True)\n",
    "\n",
    "    mask_pred_i=np.argmax(SoftmaxMean_test[obs_id_eval],axis=-1).reshape(new_H,new_W)\n",
    "    mask_std_i=np.mean(SoftmaxStd_test[obs_id_eval],axis=-1).reshape(new_H,new_W)\n",
    "\n",
    "    mask_i=mask_i\n",
    "#     mask_i[mask_i!=4]=0\n",
    "    fig, ax = plt.subplots(1,4,figsize=(16,5))\n",
    "    ax[0].imshow(img_i)\n",
    "    ax[1].imshow(img_i)\n",
    "    im1=ax[1].imshow(mask_i,cmap='Paired',vmin=0,vmax=1,alpha=0.8,interpolation='nearest')\n",
    "    ax[2].imshow(img_i)\n",
    "    ax[2].imshow(mask_pred_i,cmap='Paired',vmin=0,vmax=1,alpha=0.8,interpolation='nearest')\n",
    "    \n",
    "    ax[3].imshow(mask_std_i,cmap='Reds')\n",
    "    \n",
    "#     fig.colorbar(im1, ax=ax)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Models_eval_hands_B(SoftmaxMean,y_get_set,save_fname):\n",
    "    \n",
    "    Y_pred=np.argmax(SoftmaxMean,axis=-1).reshape(-1,new_H,new_W)\n",
    "    # precision, recall, f1\n",
    "    prc_bin, rec_bin, f1_bin, _ =precision_recall_fscore_support(y_get_set.ravel(), Y_pred.ravel())\n",
    "    # calculate entropy\n",
    "    p_c=SoftmaxMean\n",
    "    max_p=np.sum(SoftmaxMean,axis=-1)\n",
    "    max_p=np.reshape(max_p,max_p.shape+(1,))\n",
    "    p_c_norm=np.divide(p_c,max_p)\n",
    "    log_p_c=np.log10(p_c_norm+1e-12)\n",
    "    entropy=np.sum(np.multiply(-1*p_c_norm,log_p_c),axis=-1)\n",
    "    \n",
    "    # IoU\n",
    "    classes=np.unique(y_get_set)\n",
    "    nClasses=len(classes)\n",
    "    IoU_bin=[]\n",
    "\n",
    "    Mask_GT=y_get_set\n",
    "    Mask_Prd=Y_pred\n",
    "\n",
    "    for i in range(nClasses):\n",
    "        \n",
    "        GT_class_cond=(Mask_GT==classes[i])\n",
    "        Prd_class_cond=(Mask_Prd==classes[i])\n",
    "        # Measure IoU\n",
    "        intersection=np.logical_and(GT_class_cond,Prd_class_cond)\n",
    "        union=np.logical_or(GT_class_cond,Prd_class_cond)\n",
    "        \n",
    "        IoU_class = np.sum(intersection) / np.sum(union) \n",
    "        IoU_bin.append(IoU_class)    \n",
    "    \n",
    "    print('           Background    Hands       mean')\n",
    "    print('Precision:    %1.2f      %1.2f      %1.2f'%(prc_bin[0]*100,\n",
    "                                                     prc_bin[1]*100,\n",
    "                                                     np.mean(prc_bin)*100))\n",
    "    \n",
    "    print('Recall:       %1.2f      %1.2f      %1.2f'%(rec_bin[0]*100,\n",
    "                                                     rec_bin[1]*100,\n",
    "                                                     np.mean(rec_bin)*100))\n",
    "     \n",
    "    print('F1-score:     %1.2f      %1.2f      %1.2f'%(f1_bin[0]*100,\n",
    "                                                     f1_bin[1]*100,\n",
    "                                                     np.mean(f1_bin)*100))\n",
    "    \n",
    "    print('IoU:          %1.2f      %1.2f      %1.2f'%(IoU_bin[0]*100,\n",
    "                                                     IoU_bin[1]*100,\n",
    "                                                     np.mean(IoU_bin)*100))\n",
    "            \n",
    "    # save to file \n",
    "    np.savez_compressed(save_fname+'.npz',prc_bin=prc_bin, rec_bin=rec_bin, f1_bin=f1_bin, IoU_bin=IoU_bin)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Models_eval_hands_B(SoftmaxMean_test,y_gt_test,'test_metrics_SRG_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
