{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from Utility05_Bayesian import *\n",
    "# from bayes_opt import BayesianOptimization\n",
    "from tiramisu_net import *\n",
    "import scipy.io\n",
    "import cv2\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_set_id=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_di_path(path_name):\n",
    "    try:\n",
    "        os.mkdir(path_name)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % path_name)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % path_name)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of the directory case_study_models\\Model set-01 failed\n"
     ]
    }
   ],
   "source": [
    "path_name=\"case_study_models\"+'\\\\'+'Model set-'+str(Model_set_id).zfill(2)\n",
    "make_di_path(path_name)\n",
    "Model_dir=path_name+'\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "folder = Model_dir\n",
    "for the_file in os.listdir(folder):\n",
    "    file_path = os.path.join(folder, the_file)\n",
    "    try:\n",
    "        if os.path.isfile(file_path):\n",
    "            os.unlink(file_path)\n",
    "        #elif os.path.isdir(file_path): shutil.rmtree(file_path)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57, 300, 400, 3)\n",
      "(57, 300, 400)\n"
     ]
    }
   ],
   "source": [
    "filename='400x300_data.npz'\n",
    "dataset_x = np.load('400x300_data.npz')\n",
    "X_data=dataset_x['a']\n",
    "Y_data=dataset_x['b']\n",
    "print(X_data.shape)\n",
    "print(Y_data.shape)\n",
    "n_obs_Z=X_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    @staticmethod\n",
    "    def loadmodel(path):\n",
    "        return load_model(path)\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.model = self.loadmodel(path)\n",
    "        self.graph = tf.get_default_graph()\n",
    "\n",
    "    def predict(self, X):\n",
    "        with self.graph.as_default():\n",
    "            return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer zero_padding2d_1 is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [300, 400, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-ea7c2a1f3cb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mSRG_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_SRG.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-aaf51f71d4ec>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-aaf51f71d4ec>\u001b[0m in \u001b[0;36mloadmodel\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# rebuild model architecture by exporting and importing via json\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         new_model = tensorflow.keras.models.model_from_json(model.to_json(),\n\u001b[1;32m---> 11\u001b[1;33m                                                         custom_objects=None)\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m# copy weights from old model to new one\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\HRC_K231\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\model_config.py\u001b[0m in \u001b[0;36mmodel_from_json\u001b[1;34m(json_string, custom_objects)\u001b[0m\n\u001b[0;32m     94\u001b[0m   \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m  \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\HRC_K231\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m    103\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m       printable_module_name='layer')\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\HRC_K231\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    189\u001b[0m             custom_objects=dict(\n\u001b[0;32m    190\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_GLOBAL_CUSTOM_OBJECTS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m                 list(custom_objects.items())))\n\u001b[0m\u001b[0;32m    192\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\HRC_K231\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mfrom_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m   1079\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munprocessed_nodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mnode_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munprocessed_nodes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m             \u001b[0mprocess_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\HRC_K231\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mprocess_node\u001b[1;34m(layer, node_data)\u001b[0m\n\u001b[0;32m   1037\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_input_tensors\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m           \u001b[0minput_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflat_input_tensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m         \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprocess_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\HRC_K231\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[1;31m# are casted, not before.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         input_spec.assert_input_compatibility(self.input_spec, inputs,\n\u001b[1;32m--> 819\u001b[1;33m                                               self.name)\n\u001b[0m\u001b[0;32m    820\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\HRC_K231\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    175\u001b[0m                          \u001b[1;34m'expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'. Full shape received: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m                          str(x.shape.as_list()))\n\u001b[0m\u001b[0;32m    178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m       \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer zero_padding2d_1 is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [300, 400, 3]"
     ]
    }
   ],
   "source": [
    "main_model=Model('test.h5')\n",
    "SRG_model=Model('test_SRG.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Monty_Model_softmax_X(X,model, N_MCS):\n",
    "    SoftMaxBin_list=[]\n",
    "    for i in range(N_MCS):\n",
    "        SoftMaxBin_list.append(model.predict(X))\n",
    "    SoftMaxBin=np.array(SoftMaxBin_list)\n",
    "    \n",
    "    SoftmaxMean=np.mean(SoftMaxBin,axis=0)\n",
    "    SoftmaxStd=np.std(SoftMaxBin,axis=0)\n",
    "\n",
    "    return SoftmaxMean,SoftmaxStd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "N_MCS_main=20\n",
    "N_MCS_SRG=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_bin=[]\n",
    "\n",
    "alpha=0.7\n",
    "\n",
    "\n",
    "for i_obs in np.arange(0,n_obs_Z,10):\n",
    "    \n",
    "    t0 = time.time() ## initiate data loading \n",
    "    mask_i,img_i = Y_data[i_obs,:],X_data[i_obs,:]\n",
    "    img_i=img_i.reshape((1,)+img_i.shape)\n",
    "    mask_i=mask_i.reshape((1,)+mask_i.shape)\n",
    "   \n",
    "    t1 = time.time() ## initiate MCS\n",
    "    SoftmaxMean_main_i,SoftmaxStd_main_i=Monty_Model_softmax_X(img_i,main_model, N_MCS_main)\n",
    "    \n",
    "    \n",
    "    \n",
    "    t2 = time.time() ## entropy calcs\n",
    "\n",
    "    p_c=SoftmaxMean_main_i\n",
    "    max_p=np.sum(SoftmaxMean_main_i,axis=-1)\n",
    "    max_p=np.reshape(max_p,max_p.shape+(1,))\n",
    "    p_c_norm=np.divide(p_c,max_p)\n",
    "    log_p_c=np.log10(p_c_norm+1e-12)\n",
    "    entropy_i=np.sum(np.multiply(-1*p_c_norm,log_p_c),axis=-1)\n",
    "\n",
    "    t3 = time.time() ## save to file\n",
    "    SoftmaxStd_main_i=SoftmaxStd_main_i.reshape(1,data_set.new_H,data_set.new_W,2)\n",
    "    SoftmaxMean_main_i=SoftmaxMean_main_i.reshape(1,data_set.new_H,data_set.new_W,2)\n",
    "    entropy_i=entropy_i.reshape((1,data_set.new_H,data_set.new_W,1))\n",
    "    X_SRG=np.concatenate((img_i,SoftmaxMean_main_i,SoftmaxStd_main_i,entropy_i),axis=-1) \n",
    "\n",
    "    y_pred_main=np.argmax(SoftmaxMean_main_i,axis=-1)\n",
    "    \n",
    "    # np.savez_compressed(set_dir+'/X_SRG_'+str(i_obs+1)+'.npz',X_SRG=X_SRG)\n",
    "\n",
    "    t4 = time.time() ## Initiate second inference\n",
    "    \n",
    "    SoftmaxMean_SRG_i,SoftmaxStd_SRG_i=Monty_Model_softmax_X(X_SRG,SRG_model, N_MCS_SRG)\n",
    "    \n",
    "    y_pred_SRG=np.argmax(SoftmaxMean_SRG_i,axis=-1).reshape(1,data_set.new_H,data_set.new_W)\n",
    "    \n",
    "    t5 = time.time() ## End of inference \n",
    "    \n",
    "    # times\n",
    "    dt_1=t1-t0  # load data\n",
    "    dt_2=t2-t1  # perform MCS\n",
    "    dt_3=t3-t2  # measure entropy\n",
    "    dt_4=t4-t3  # Build SRG input\n",
    "    dt_5=t5-t4  # SRG inference\n",
    "    t_sum=t5-t0\n",
    "    \n",
    "    t_bin.append([dt_1,dt_2,dt_3,dt_4,dt_5])\n",
    "    print('Obs. %d processed, t_load: %1.3f, t_MCS: %1.3f, t_ENTP: %1.3f, t_inSRG: %1.3f, t_SRG: %1.3f t_sum= %1.3f (s)'%((i_obs+1,dt_1,dt_2,dt_3,dt_4,dt_5,t_sum)))\n",
    "    \n",
    "    fig,ax = plt.subplots(1,6,figsize=(18,3))\n",
    "    plt.subplots_adjust(wspace=0.12, hspace=0.1)\n",
    "    \n",
    "    for i in range(6):\n",
    "        ax[i].spines['top'].set_visible(1.2)\n",
    "        ax[i].spines['right'].set_visible(1.2)\n",
    "        ax[i].spines['bottom'].set_linewidth(1.2)\n",
    "        ax[i].spines['left'].set_linewidth(1.2)\n",
    "        ax[i].set_yticklabels([])\n",
    "        ax[i].set_xticklabels([])\n",
    "        ax[i].tick_params(bottom=False, left=False)\n",
    "    \n",
    "    ax[0].imshow(img_i[0])\n",
    "    ax[0].set_xlabel('image')\n",
    "\n",
    "    ax[1].imshow(img_i[0])\n",
    "    ax[1].imshow(mask_i[0],alpha=alpha,cmap='GnBu')\n",
    "    ax[1].set_xlabel('GT')\n",
    "    \n",
    "    ax[2].imshow(img_i[0])\n",
    "    ax[2].imshow(y_pred_main[0],alpha=alpha,cmap='GnBu')\n",
    "    ax[2].set_xlabel('main pred')\n",
    "\n",
    "    ax[3].imshow(img_i[0])\n",
    "    ax[3].imshow(y_pred_SRG[0],alpha=alpha,cmap='GnBu')\n",
    "    ax[3].set_xlabel('SRG pred')\n",
    "    \n",
    "    ax[4].imshow(img_i[0])\n",
    "    ax[4].imshow(SoftmaxStd_main_i[0,:,:,1],alpha=alpha,cmap='OrRd')\n",
    "    ax[4].set_xlabel('Std')    \n",
    "    \n",
    "    ax[5].imshow(img_i[0])\n",
    "    ax[5].imshow(entropy_i[0,:,:,0],alpha=alpha,cmap='OrRd')\n",
    "    ax[5].set_xlabel('Entropy')    \n",
    "    \n",
    "    \n",
    "    plot_save_name='test_save/'+str(i_obs+1).zfill(3)+'.png'\n",
    "    fig.savefig(plot_save_name,dpi=150, pad_inches=0.1,bbox_inches='tight')   \n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(mask_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i_obs in np.arange(0,800,10):\n",
    "    obs_id_eval=i_obs\n",
    "\n",
    "    mask_i,img_i = test_set.load_data(obs_id_eval,verbose=0,resize=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    mask_pred_i=np.argmax(SoftmaxMean_test[obs_id_eval],axis=-1).reshape(new_H,new_W)\n",
    "    mask_std_i=np.mean(SoftmaxStd_test[obs_id_eval],axis=-1).reshape(new_H,new_W)\n",
    "\n",
    "    mask_i=mask_i-1\n",
    "    mask_i[mask_i>1]=1\n",
    "    fig, ax = plt.subplots(1,4,figsize=(16,5))\n",
    "    ax[0].imshow(img_i)\n",
    "    ax[1].imshow(mask_i,cmap='Blues')\n",
    "    ax[2].imshow(mask_pred_i)\n",
    "    ax[3].imshow(mask_std_i,cmap='Reds')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Models_eval_hands_B(SoftmaxMean,y_get_set,save_fname):\n",
    "    \n",
    "    Y_pred=np.argmax(SoftmaxMean,axis=-1).reshape(-1,new_H,new_W)\n",
    "    # precision, recall, f1\n",
    "    prc_bin, rec_bin, f1_bin, _ =precision_recall_fscore_support(y_get_set.ravel(), Y_pred.ravel())\n",
    "    # calculate entropy\n",
    "    p_c=SoftmaxMean\n",
    "    max_p=np.sum(SoftmaxMean,axis=-1)\n",
    "    max_p=np.reshape(max_p,max_p.shape+(1,))\n",
    "    p_c_norm=np.divide(p_c,max_p)\n",
    "    log_p_c=np.log10(p_c_norm+1e-12)\n",
    "    entropy=np.sum(np.multiply(-1*p_c_norm,log_p_c),axis=-1)\n",
    "    \n",
    "    # IoU\n",
    "    classes=np.unique(y_get_set)\n",
    "    nClasses=len(classes)\n",
    "    IoU_bin=[]\n",
    "\n",
    "    Mask_GT=y_get_set\n",
    "    Mask_Prd=Y_pred\n",
    "\n",
    "    for i in range(nClasses):\n",
    "        \n",
    "        GT_class_cond=(Mask_GT==classes[i])\n",
    "        Prd_class_cond=(Mask_Prd==classes[i])\n",
    "        # Measure IoU\n",
    "        intersection=np.logical_and(GT_class_cond,Prd_class_cond)\n",
    "        union=np.logical_or(GT_class_cond,Prd_class_cond)\n",
    "        \n",
    "        IoU_class = np.sum(intersection) / np.sum(union) \n",
    "        IoU_bin.append(IoU_class)    \n",
    "    \n",
    "    print('           Background    Hands')\n",
    "    print('Precision:    %1.4f      %1.4f'%(prc_bin[0]*100,prc_bin[1]*100))\n",
    "    print('Recall:       %1.4f      %1.4f'%(rec_bin[0]*100,rec_bin[1]*100))\n",
    "    print('F1-score:     %1.4f      %1.4f'%(f1_bin[0]*100,f1_bin[1]*100))\n",
    "    print('IoU:          %1.4f      %1.4f'%(IoU_bin[0]*100,IoU_bin[1]*100))\n",
    "            \n",
    "    # save to file \n",
    "    np.savez_compressed(save_fname+'.npz',prc_bin=prc_bin, rec_bin=rec_bin, f1_bin=f1_bin, IoU_bin=IoU_bin)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Models_eval_hands_B(SoftmaxMean_test,y_gt_test,'test_metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
