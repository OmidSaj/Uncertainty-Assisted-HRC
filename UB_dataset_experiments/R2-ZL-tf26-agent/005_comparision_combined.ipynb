{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Utility05_Bayesian import *\n",
    "# from bayes_opt import BayesianOptimization\n",
    "from tiramisu_net import *\n",
    "import scipy.io\n",
    "import cv2\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_set_id=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_di_path(path_name):\n",
    "    try:\n",
    "        os.mkdir(path_name)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % path_name)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % path_name)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of the directory case_study_models\\Model set-01 failed\n"
     ]
    }
   ],
   "source": [
    "path_name=\"case_study_models\"+'\\\\'+'Model set-'+str(Model_set_id).zfill(2)\n",
    "make_di_path(path_name)\n",
    "Model_dir=path_name+'\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "folder = Model_dir\n",
    "for the_file in os.listdir(folder):\n",
    "    file_path = os.path.join(folder, the_file)\n",
    "    try:\n",
    "        if os.path.isfile(file_path):\n",
    "            os.unlink(file_path)\n",
    "        #elif os.path.isdir(file_path): shutil.rmtree(file_path)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_H=180\n",
    "new_W=320\n",
    "N_MCS=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'MFW_data.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36480/3962317415.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mMFW_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'MFW_data.npz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpixel_unique\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMFW_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pixel_unique'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpixel_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMFW_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pixel_count'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclass_weights_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMFW_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class_weights_freq'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf26\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m             \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'MFW_data.npz'"
     ]
    }
   ],
   "source": [
    "MFW_data=np.load('MFW_data.npz')\n",
    "\n",
    "pixel_unique=MFW_data['pixel_unique']\n",
    "pixel_count=MFW_data['pixel_count']\n",
    "class_weights_freq=MFW_data['class_weights_freq']\n",
    "\n",
    "print('pixel_unique:')\n",
    "print(pixel_unique)\n",
    "\n",
    "print('pixel_count:')\n",
    "pixel_count[1]=sum(pixel_count[1:])\n",
    "pixel_count=pixel_count[0:2]\n",
    "print(pixel_count)\n",
    "pixel_med=np.median(pixel_count)\n",
    "class_weights_freq=pixel_med/pixel_count\n",
    "\n",
    "print('class_weights_freq:')\n",
    "print(class_weights_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ego_data_split:\n",
    "    \n",
    "    def __init__(self,set_dir,n_obs,new_H=new_H,new_W=new_W):\n",
    "        self.n_obs = n_obs\n",
    "        self.set_dir = set_dir\n",
    "        self.new_H = new_H\n",
    "        self.new_W = new_W        \n",
    "\n",
    "    def resize_data(data):\n",
    "        \n",
    "        return data_res\n",
    "    \n",
    "    def load_data(self,image_id, resize=True,verbose=0,\n",
    "                  norm_RGB=255):\n",
    "        mask_i = scipy.io.loadmat(self.set_dir+'/hand_mask_'+str(image_id+1)+'.mat')['hand_mask']\n",
    "        img_i = scipy.io.loadmat(self.set_dir+'/img_'+str(image_id+1)+'.mat')['img']/norm_RGB\n",
    "        \n",
    "        if resize:\n",
    "            mask_i = cv2.resize(np.array(mask_i), (self.new_W,self.new_H), interpolation =cv2.INTER_NEAREST)\n",
    "            img_i = cv2.resize(np.array(img_i), (self.new_W,self.new_H), interpolation =cv2.INTER_NEAREST)\n",
    "            \n",
    "        if verbose==1:\n",
    "            print('img_i.shape:  '+str(mask_i.shape))\n",
    "            print('mask_i.shape: '+str(mask_i.shape))\n",
    "            \n",
    "        return mask_i,img_i    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set=Ego_data_split('../../dataset/01_train',3600)\n",
    "val_set=Ego_data_split('../../dataset/02_val',400)\n",
    "test_set=Ego_data_split('../../dataset/03_test',800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, set_dir, n_obs, class_weights, batch_size,\n",
    "                 n_classes=2, shuffle=True, new_H=new_H, new_W=new_W):\n",
    "        'Initialization'\n",
    "        self.set_dir = set_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.class_weights = class_weights\n",
    "        self.n_obs = n_obs\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        self.new_H = new_H\n",
    "        self.new_W = new_W\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(self.n_obs / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "        data_tuple = self.__data_generation(indexes)\n",
    "\n",
    "        return data_tuple\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.n_obs)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, self.new_H,self.new_W, 3))\n",
    "        y = np.empty((self.batch_size, self.new_H,self.new_W), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(indexes):\n",
    "            # Store sample\n",
    "            img_i = scipy.io.loadmat(self.set_dir+'/img_'+str(ID+1)+'.mat')['img']/255\n",
    "            X[i,] = cv2.resize(np.array(img_i), (self.new_W,self.new_H), interpolation =cv2.INTER_NEAREST)\n",
    "            \n",
    "            # Store class\n",
    "            mask_i = scipy.io.loadmat(self.set_dir+'/hand_mask_'+str(ID+1)+'.mat')['hand_mask']\n",
    "            y[i] = cv2.resize(np.array(mask_i), (self.new_W,self.new_H), interpolation =cv2.INTER_NEAREST)-1\n",
    "         \n",
    "        y[y>1]=1\n",
    "        y_resh=y.reshape((-1,self.new_H*self.new_W))\n",
    "#         print(np.amax(y_resh))\n",
    "#         print(y_resh.shape)\n",
    "        y_cat=keras.utils.to_categorical(y_resh, num_classes=self.n_classes)\n",
    "        batch_wieghts = gen_weight_mat(y,self.class_weights)\n",
    "        \n",
    "\n",
    "        return (X, y_cat,batch_wieghts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph1 = tf.Graph()\n",
    "# with graph1.as_default():\n",
    "#     session1 = tf.Session()\n",
    "#     with session1.as_default():\n",
    "#         main_model=load_model('test.h5')\n",
    "# print('main model loaded')    \n",
    "\n",
    "# graph2 = tf.Graph()\n",
    "# with graph2.as_default():\n",
    "#     session2 = tf.Session()\n",
    "#     with session2.as_default():\n",
    "#         SRG_model=load_model('test_SRG.h5')\n",
    "        \n",
    "# print('SRG model loaded') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    @staticmethod\n",
    "    def loadmodel(path):\n",
    "        return load_model(path)\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.model = self.loadmodel(path)\n",
    "        self.graph = tf.get_default_graph()\n",
    "\n",
    "    def predict(self, X):\n",
    "        with self.graph.as_default():\n",
    "            return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_model=Model('test.h5')\n",
    "SRG_model=Model('test_SRG_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Monty_Model_softmax_X(X,model, N_MCS):\n",
    "    SoftMaxBin_list=[]\n",
    "    for i in range(N_MCS):\n",
    "        SoftMaxBin_list.append(model.predict(X))\n",
    "    SoftMaxBin=np.array(SoftMaxBin_list)\n",
    "    \n",
    "    SoftmaxMean=np.mean(SoftMaxBin,axis=0)\n",
    "    SoftmaxStd=np.std(SoftMaxBin,axis=0)\n",
    "\n",
    "    return SoftmaxMean,SoftmaxStd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def get_Y(set_dir,new_H=new_H,new_W=new_W):\n",
    "    obs_dir=glob.glob(set_dir+'/hand_mask_*')\n",
    "    n_obs_set=len(obs_dir)\n",
    "    y=np.empty((n_obs_set, new_H,new_W), dtype=int)\n",
    "    for i_obs in range(n_obs_set):\n",
    "        mask_i = scipy.io.loadmat(set_dir+'/hand_mask_'+str(i_obs+1)+'.mat')['hand_mask']\n",
    "        y[i_obs] = cv2.resize(np.array(mask_i), (new_W,new_H), interpolation =cv2.INTER_NEAREST)-1\n",
    "\n",
    "    y[y>1]=1\n",
    "#     y_resh=y.reshape((-1,new_H*new_W))\n",
    "    print(y.shape)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_gt_test=get_Y('../../dataset/03_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "N_MCS_main=20\n",
    "N_MCS_SRG=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_set=test_set\n",
    "\n",
    "t_bin=[]\n",
    "\n",
    "alpha=0.7\n",
    "\n",
    "\n",
    "for i_obs in np.arange(0,800,10):\n",
    "    \n",
    "    t0 = time.time() ## initiate data loading \n",
    "    mask_i,img_i = data_set.load_data(i_obs,verbose=0,resize=True)\n",
    "    img_i=img_i.reshape((1,)+img_i.shape)\n",
    "    mask_i=mask_i.reshape((1,)+mask_i.shape)\n",
    "\n",
    "    mask_i[mask_i>2]=2\n",
    "    \n",
    "    t1 = time.time() ## initiate MCS\n",
    "    SoftmaxMean_main_i,SoftmaxStd_main_i=Monty_Model_softmax_X(img_i,main_model, N_MCS_main)\n",
    "    \n",
    "    \n",
    "    \n",
    "    t2 = time.time() ## entropy calcs\n",
    "\n",
    "    p_c=SoftmaxMean_main_i\n",
    "    max_p=np.sum(SoftmaxMean_main_i,axis=-1)\n",
    "    max_p=np.reshape(max_p,max_p.shape+(1,))\n",
    "    p_c_norm=np.divide(p_c,max_p)\n",
    "    log_p_c=np.log10(p_c_norm+1e-12)\n",
    "    entropy_i=np.sum(np.multiply(-1*p_c_norm,log_p_c),axis=-1)\n",
    "\n",
    "    t3 = time.time() ## save to file\n",
    "    SoftmaxStd_main_i=SoftmaxStd_main_i.reshape(1,data_set.new_H,data_set.new_W,2)\n",
    "    SoftmaxMean_main_i=SoftmaxMean_main_i.reshape(1,data_set.new_H,data_set.new_W,2)\n",
    "    entropy_i=entropy_i.reshape((1,data_set.new_H,data_set.new_W,1))\n",
    "    X_SRG=np.concatenate((img_i,SoftmaxMean_main_i,SoftmaxStd_main_i,entropy_i),axis=-1) \n",
    "\n",
    "    y_pred_main=np.argmax(SoftmaxMean_main_i,axis=-1)\n",
    "    \n",
    "    # np.savez_compressed(set_dir+'/X_SRG_'+str(i_obs+1)+'.npz',X_SRG=X_SRG)\n",
    "\n",
    "    t4 = time.time() ## Initiate second inference\n",
    "    \n",
    "    SoftmaxMean_SRG_i,SoftmaxStd_SRG_i=Monty_Model_softmax_X(X_SRG,SRG_model, N_MCS_SRG)\n",
    "    \n",
    "    y_pred_SRG=np.argmax(SoftmaxMean_SRG_i,axis=-1).reshape(1,data_set.new_H,data_set.new_W)\n",
    "    \n",
    "    t5 = time.time() ## End of inference \n",
    "    \n",
    "    # times\n",
    "    dt_1=t1-t0  # load data\n",
    "    dt_2=t2-t1  # perform MCS\n",
    "    dt_3=t3-t2  # measure entropy\n",
    "    dt_4=t4-t3  # Build SRG input\n",
    "    dt_5=t5-t4  # SRG inference\n",
    "    t_sum=t5-t0\n",
    "    \n",
    "    t_bin.append([dt_1,dt_2,dt_3,dt_4,dt_5])\n",
    "    print('Obs. %d processed, t_load: %1.3f, t_MCS: %1.3f, t_ENTP: %1.3f, t_inSRG: %1.3f, t_SRG: %1.3f t_sum= %1.3f (s)'%((i_obs+1,dt_1,dt_2,dt_3,dt_4,dt_5,t_sum)))\n",
    "    \n",
    "    fig,ax = plt.subplots(1,6,figsize=(18,3))\n",
    "    plt.subplots_adjust(wspace=0.12, hspace=0.1)\n",
    "    \n",
    "    for i in range(6):\n",
    "        ax[i].spines['top'].set_visible(1.2)\n",
    "        ax[i].spines['right'].set_visible(1.2)\n",
    "        ax[i].spines['bottom'].set_linewidth(1.2)\n",
    "        ax[i].spines['left'].set_linewidth(1.2)\n",
    "        ax[i].set_yticklabels([])\n",
    "        ax[i].set_xticklabels([])\n",
    "        ax[i].tick_params(bottom=False, left=False)\n",
    "    \n",
    "    ax[0].imshow(img_i[0])\n",
    "    ax[0].set_xlabel('image')\n",
    "\n",
    "    ax[1].imshow(img_i[0])\n",
    "    ax[1].imshow(mask_i[0],alpha=alpha,cmap='GnBu')\n",
    "    ax[1].set_xlabel('GT')\n",
    "    \n",
    "    ax[2].imshow(img_i[0])\n",
    "    ax[2].imshow(y_pred_main[0],alpha=alpha,cmap='GnBu')\n",
    "    ax[2].set_xlabel('main pred')\n",
    "\n",
    "    ax[3].imshow(img_i[0])\n",
    "    ax[3].imshow(y_pred_SRG[0],alpha=alpha,cmap='GnBu')\n",
    "    ax[3].set_xlabel('SRG pred')\n",
    "    \n",
    "    ax[4].imshow(img_i[0])\n",
    "    ax[4].imshow(SoftmaxStd_main_i[0,:,:,1],alpha=alpha,cmap='OrRd')\n",
    "    ax[4].set_xlabel('Std')    \n",
    "    \n",
    "    ax[5].imshow(img_i[0])\n",
    "    ax[5].imshow(entropy_i[0,:,:,0],alpha=alpha,cmap='OrRd')\n",
    "    ax[5].set_xlabel('Entropy')    \n",
    "    \n",
    "    \n",
    "    plot_save_name='test_save/'+str(i_obs+1).zfill(3)+'.png'\n",
    "    fig.savefig(plot_save_name,dpi=150, pad_inches=0.1,bbox_inches='tight',facecolor='white')   \n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(mask_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class DataGenerator_B(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, set_dir, class_weights, batch_size,\n",
    "                 ind_first=0,\n",
    "                 ind_last=1799,\n",
    "                 n_classes=2, shuffle=True, new_H=new_H, new_W=new_W):\n",
    "        'Initialization'\n",
    "        self.set_dir = set_dir\n",
    "        self.ind_first= ind_first\n",
    "        self.ind_last= ind_last\n",
    "        self.batch_size = batch_size\n",
    "        self.class_weights = class_weights\n",
    "        self.n_obs = self.ind_last-self.ind_first+1\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        self.new_H = new_H\n",
    "        self.new_W = new_W\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(self.n_obs / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "        data_tuple = self.__data_generation(indexes)\n",
    "\n",
    "        return data_tuple\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.ind_first,self.ind_last+1)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, self.new_H,self.new_W, 8))\n",
    "        y = np.empty((self.batch_size, self.new_H,self.new_W), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(indexes):\n",
    "            # Store sample\n",
    "            X[i,] = np.load(self.set_dir+'/X_SRG_'+str(ID+1)+'.npz')['X_SRG'] # already normalized\n",
    "            \n",
    "            # Store class\n",
    "            mask_i = scipy.io.loadmat(self.set_dir+'/hand_mask_'+str(ID+1)+'.mat')['hand_mask']\n",
    "            y[i] = cv2.resize(np.array(mask_i), (self.new_W,self.new_H), interpolation =cv2.INTER_NEAREST)-1\n",
    "         \n",
    "        y[y>1]=1\n",
    "        y_resh=y.reshape((-1,self.new_H*self.new_W))\n",
    "#         print(np.amax(y_resh))\n",
    "#         print(y_resh.shape)\n",
    "        y_cat=keras.utils.to_categorical(y_resh, num_classes=self.n_classes)\n",
    "        batch_wieghts = gen_weight_mat(y,self.class_weights)\n",
    "        \n",
    "\n",
    "        return (X, y_cat,batch_wieghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_eval = 40\n",
    "test_generator = DataGenerator_B('../../dataset/03_test',\n",
    "                               class_weights_freq,batch_size_eval,\n",
    "                               ind_first=0,\n",
    "                               ind_last=799,\n",
    "                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Monty_Model_softmax_SRG(data_generator, model, N_MCS=N_MCS,steps=int(800/batch_size_eval)):\n",
    "    SoftMaxBin_list=[]\n",
    "    for i in range(N_MCS):\n",
    "        print(i)\n",
    "        SoftMaxBin_list.append(model.model.predict_generator(data_generator,steps=steps))\n",
    "    SoftMaxBin=np.array(SoftMaxBin_list)\n",
    "    SoftmaxMean=np.mean(SoftMaxBin,axis=0)\n",
    "    SoftmaxStd=np.std(SoftMaxBin,axis=0)\n",
    "    return SoftmaxMean,SoftmaxStd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SoftmaxMean_test_SRG,SoftmaxStd_test_SRG = Monty_Model_softmax_SRG(test_generator,SRG_model, N_MCS=N_MCS_SRG,steps=int(800/batch_size_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Models_eval_hands_B(SoftmaxMean,y_get_set,save_fname):\n",
    "    \n",
    "    Y_pred=np.argmax(SoftmaxMean,axis=-1).reshape(-1,new_H,new_W)\n",
    "    # precision, recall, f1\n",
    "    prc_bin, rec_bin, f1_bin, _ =precision_recall_fscore_support(y_get_set.ravel(), Y_pred.ravel())\n",
    "    # calculate entropy\n",
    "    p_c=SoftmaxMean\n",
    "    max_p=np.sum(SoftmaxMean,axis=-1)\n",
    "    max_p=np.reshape(max_p,max_p.shape+(1,))\n",
    "    p_c_norm=np.divide(p_c,max_p)\n",
    "    log_p_c=np.log10(p_c_norm+1e-12)\n",
    "    entropy=np.sum(np.multiply(-1*p_c_norm,log_p_c),axis=-1)\n",
    "    \n",
    "    # IoU\n",
    "    classes=np.unique(y_get_set)\n",
    "    nClasses=len(classes)\n",
    "    IoU_bin=[]\n",
    "\n",
    "    Mask_GT=y_get_set\n",
    "    Mask_Prd=Y_pred\n",
    "\n",
    "    for i in range(nClasses):\n",
    "        \n",
    "        GT_class_cond=(Mask_GT==classes[i])\n",
    "        Prd_class_cond=(Mask_Prd==classes[i])\n",
    "        # Measure IoU\n",
    "        intersection=np.logical_and(GT_class_cond,Prd_class_cond)\n",
    "        union=np.logical_or(GT_class_cond,Prd_class_cond)\n",
    "        \n",
    "        IoU_class = np.sum(intersection) / np.sum(union) \n",
    "        IoU_bin.append(IoU_class)    \n",
    "    \n",
    "    print('           Background    Hands')\n",
    "    print('Precision:    %1.4f      %1.4f'%(prc_bin[0]*100,prc_bin[1]*100))\n",
    "    print('Recall:       %1.4f      %1.4f'%(rec_bin[0]*100,rec_bin[1]*100))\n",
    "    print('F1-score:     %1.4f      %1.4f'%(f1_bin[0]*100,f1_bin[1]*100))\n",
    "    print('IoU:          %1.4f      %1.4f'%(IoU_bin[0]*100,IoU_bin[1]*100))\n",
    "            \n",
    "    # save to file \n",
    "    np.savez_compressed(save_fname+'.npz',prc_bin=prc_bin, rec_bin=rec_bin, f1_bin=f1_bin, IoU_bin=IoU_bin)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def get_Y(set_dir,new_H=new_H,new_W=new_W):\n",
    "    obs_dir=glob.glob(set_dir+'/hand_mask_*')\n",
    "    n_obs_set=len(obs_dir)\n",
    "    y=np.empty((n_obs_set, new_H,new_W), dtype=int)\n",
    "    for i_obs in range(n_obs_set):\n",
    "        mask_i = scipy.io.loadmat(set_dir+'/hand_mask_'+str(i_obs+1)+'.mat')['hand_mask']\n",
    "        y[i_obs] = cv2.resize(np.array(mask_i), (new_W,new_H), interpolation =cv2.INTER_NEAREST)-1\n",
    "\n",
    "    y[y>1]=1\n",
    "#     y_resh=y.reshape((-1,new_H*new_W))\n",
    "    print(y.shape)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gt_test=get_Y('../../dataset/03_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Models_eval_hands_B(SoftmaxMean_test_SRG,y_gt_test,'test_metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
